Example: Loading, Preprocessing, Circular CV, ModelPipelineDF Selection
=============================================================

Some imports:

.. code:: ipython3

    import yieldengine.core
    from yieldengine.loading.sample import Sample
    from yieldengine.preprocessing.cross_validation import CircularCrossValidator
    from yieldengine.modeling.model_selector import ModelSelector
    from sklearn.pipeline import Pipeline
    import numpy as np
    import pandas as pd
    from sklearn.preprocessing import OneHotEncoder
    from sklearn.impute import SimpleImputer
    from sklearn.compose import ColumnTransformer
    from lightgbm.sklearn import LGBMRegressor
    from sklearn.tree import DecisionTreeRegressor, ExtraTreeRegressor
    from sklearn.ensemble import AdaBoostRegressor, RandomForestRegressor
    from sklearn.svm import SVR
    from sklearn.linear_model import LinearRegression
    from sklearn.model_selection import GridSearchCV
    from sklearn.metrics import make_scorer, mean_squared_error
    import os
    pd.set_option('display.max_columns', 10)

Loading the data file into Pandas:

.. code:: ipython3

    # load the data file
    import tests.testdata
    
    data_folder_path = os.path.dirname(tests.testdata.__file__)
    
    # Note: this file is not included within the git repository!
    testdata_file_path = os.path.join(
        data_folder_path, "master_table_clean_anon_144.csv"
    )
    
    inputfile_config = yieldengine.core.get_global_config(section="inputfile")
    data_file = pd.read_csv(
        filepath_or_buffer=testdata_file_path,
        delimiter=inputfile_config["delimiter"],
        header=inputfile_config["header"],
        decimal=inputfile_config["decimal"],
    )

Some very basic clean up specific to this data file:

.. code:: ipython3

    data_file = data_file.drop(columns=["Date", "Batch Id"])
    
    # replace values of +/- infinite with n/a, then drop all n/a columns:
    data_file = data_file.replace([np.inf, -np.inf], np.nan).dropna(
        axis=1, how="all"
    )
    data_file.head()




.. raw:: html

    <div>
    <style scoped>
        .dataframe {
            font-size:8pt;
        }
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
            font-size:8pt;
        }
    
        .dataframe tbody tr th {
            vertical-align: top;
            font-size:8pt;

        }
    
        .dataframe thead th {
            text-align: right;
            font-size:8pt;

        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>Step4 Fermentation Sensor Data Phase2 Pressure Val04 (mbar)</th>
          <th>Step4-6 RawMat Vendor Compound09 Purity Loss on Drying (g)</th>
          <th>Step6 Fermentation Sensor Data Phase3 Agitation Speed Val03 No1 (rounds per minutes)</th>
          <th>Step2 RawMat Internal Compound15 Number Openings (#)</th>
          <th>Step3 Media Age at Inoculation (days)</th>
          <th>...</th>
          <th>Step6 Fermentation Sensor Data Phase3 Temperature Val03 (°C)</th>
          <th>Step6 Media Compound20 Age at Inoculation (days)</th>
          <th>Step4 RawMat Vendor Compound12 Manufacturing Date (numeric date)</th>
          <th>Step5 Fermentation Wait Duration (minutes)</th>
          <th>Step4-6 RawMat Internal Compound04 Age (days)</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>0</th>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>13.0</td>
          <td>21.0</td>
          <td>...</td>
          <td>NaN</td>
          <td>3.0</td>
          <td>80963.0</td>
          <td>767.0</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>1</th>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>13.0</td>
          <td>21.0</td>
          <td>...</td>
          <td>NaN</td>
          <td>3.0</td>
          <td>80963.0</td>
          <td>767.0</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>2</th>
          <td>NaN</td>
          <td>NaN</td>
          <td>NaN</td>
          <td>13.0</td>
          <td>21.0</td>
          <td>...</td>
          <td>NaN</td>
          <td>3.0</td>
          <td>80963.0</td>
          <td>767.0</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>3</th>
          <td>790.284723</td>
          <td>NaN</td>
          <td>417.871938</td>
          <td>13.0</td>
          <td>25.0</td>
          <td>...</td>
          <td>68.835293</td>
          <td>4.0</td>
          <td>80963.0</td>
          <td>727.0</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>4</th>
          <td>783.702715</td>
          <td>NaN</td>
          <td>413.294366</td>
          <td>13.0</td>
          <td>25.0</td>
          <td>...</td>
          <td>68.764230</td>
          <td>4.0</td>
          <td>80963.0</td>
          <td>727.0</td>
          <td>NaN</td>
        </tr>
      </tbody>
    </table>
    <p>5 rows × 142 columns</p>
    </div>


Use the utility class `Sample <./../yieldengine.loading.html#yieldengine.loading.sample.Sample>`_ with this data set:

.. code:: ipython3

    feature_columns = list(data_file.columns)
    feature_columns.remove("Yield")

    sample = Sample(sample=data_file, target="Yield", features=feature_columns)

Use the yield-engine `CircularCrossValidator <./../yieldengine.preprocessing.html#yieldengine.preprocessing.cross_validation.CircularCrossValidator>`_:

.. code:: ipython3

    circular_cv = CircularCrossValidator(
        num_samples=len(sample), test_ratio=0.20, num_folds=5
    )


Define a basic scikit-learn ColumnTransformer for preprocessing, consisting of:

    -   a SimpleImputer for all numerical features
    -   a OneHotEncoder for all categorical features

Note how `Sample <./../yieldengine.loading.html#yieldengine.loading.sample.Sample>`_ is leveraged to select the specific feature types:

.. code:: ipython3

    preprocessor = ColumnTransformer(
        [
            ("numerical", SimpleImputer(strategy="mean"), sample.numerical_features),
            (
                "categorical",
                OneHotEncoder(sparse=False, handle_unknown="ignore"),
                sample.categorical_features,
            ),
        ]
    )

...wrapping it in a scikit-learn Pipeline:

.. code:: ipython3

    pre_pipeline = Pipeline([("prep", preprocessor)])

Now we define the "model zoo" - a list of tuples of any ML regressor that implements scikit-learn's API,
coupled with corresponding hyperparameters for it:

.. code:: ipython3

    searchers = []
    
    models_and_parameters = [
        (
            LGBMRegressor(),
            {
                "max_depth": (5, 10),
                "min_split_gain": (0.1, 0.2),
                "num_leaves": (50, 100, 200),
            },
        ),
        (AdaBoostRegressor(), {"n_estimators": (50, 80)}),
        (RandomForestRegressor(), {"n_estimators": (50, 80)}),
        (
            DecisionTreeRegressor(),
            {"max_depth": (0.5, 1.0), "max_features": (0.5, 1.0)},
        ),
        (ExtraTreeRegressor(), {"max_depth": (5, 10, 12)}),
        (SVR(), {"gamma": (0.5, 1), "C": (50, 100)}),
        (LinearRegression(), {"normalize": (False, True)}),
    ]

Now iterate over the model and parameter tupels, creating a **GridSearchCV** instance
for each, which in turn leverages the `CircularCrossValidator <./../yieldengine.preprocessing.html#yieldengine.preprocessing.cross_validation.CircularCrossValidator>`_ defined above:

.. code:: ipython3

    for model, parameters in models_and_parameters:
        search = GridSearchCV(
            estimator=model,
            cv=circular_cv,
            param_grid=parameters,
            scoring=make_scorer(mean_squared_error, greater_is_better=False),
            n_jobs=-1,
        )
        searchers.append(search)

Finally, use yield-engine's `ModelSelector <./../yieldengine.modeling.html#yieldengine.modeling.model_selector.ModelSelector>`_ class to train and validate all models:

.. code:: ipython3

    # instantiate the model selector
    ms = ModelSelector(searchers=searchers, preprocessing=pre_pipeline)
    
    # retrieve a pipeline
    complete_pipeline = ms.construct_pipeline()

    # train the models
    complete_pipeline.fit(sample.feature_data, sample.target_data)


Print the summary:

.. code:: ipython3

    print(ms.summary_string())


.. parsed-literal::

     Rank 1: <class 'lightgbm.sklearn.LGBMRegressor'>, Score: 2.930346873826561, Params: {'max_depth': 5, 'min_split_gain': 0.2, 'num_leaves': 50}
     Rank 2: <class 'sklearn.ensemble.weight_boosting.AdaBoostRegressor'>, Score: 3.0880021277758356, Params: {'n_estimators': 50}
     Rank 3: <class 'sklearn.ensemble.forest.RandomForestRegressor'>, Score: 3.2029551075144513, Params: {'n_estimators': 50}
     Rank 4: <class 'sklearn.tree.tree.DecisionTreeRegressor'>, Score: 3.815605624910055, Params: {'max_depth': 1.0, 'max_features': 0.5}
     Rank 5: <class 'sklearn.svm.classes.SVR'>, Score: 4.130979213835395, Params: {'C': 100, 'gamma': 0.5}
     Rank 6: <class 'sklearn.tree.tree.ExtraTreeRegressor'>, Score: 4.148205120022545, Params: {'max_depth': 5}
     Rank 7: <class 'sklearn.linear_model.base.LinearRegression'>, Score: 1958.5687650690813, Params: {'normalize': False}

Or as a ranked list:

.. code:: ipython3

    ms.rank_models()